{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'はずす'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from converter import *\n",
    "romajiToJapanese('hazusu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# URL_LVL_DICT_RAD,URL_LVL_DICT_KAN,URL_LVL_DICT_VOC={},{},{}\n",
    "# for i in tqdm(range(1,61)):\n",
    "#     soup = BeautifulSoup(urllib.request.urlopen(f\"http://wanikani.com/level/{i}\") )\n",
    "#     for j in soup.find_all('a', {'href': re.compile(r'radicals/')}):\n",
    "#         URL_LVL_DICT_RAD[i]=URL_LVL_DICT_RAD.get(i,[])+['http://wanikani.com'+j['href']]\n",
    "#     for j in soup.find_all('a', {'href': re.compile(r'kanji/')}):\n",
    "#         URL_LVL_DICT_KAN[i]=URL_LVL_DICT_KAN.get(i,[])+['http://wanikani.com'+j['href']]\n",
    "#     for j in soup.find_all('a', {'href': re.compile(r'vocabulary/')}):\n",
    "#         URL_LVL_DICT_VOC[i]=URL_LVL_DICT_VOC.get(i,[])+['http://wanikani.com'+j['href']]\n",
    "\n",
    "with open('urls.pkl', 'rb') as inp:\n",
    "    URL_LVL_DICT_RAD,URL_LVL_DICT_KAN,URL_LVL_DICT_VOC=pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_rads={1: [8, 9, 20], 3: [8, 18], 4: [3, 13, 28], 5: [9, 18, 22], 8: [1], 9: [3, 8, 14, 16, 17, 19], 10: [3], 11: [0, 1, 12], 12: [10], 13: [4], 14: [0], 15: [0, 5], 16: [4], 20: [0], 23: [3], 24: [3], 26: [2], 30: [0], 32: [3], 33: [0], 41: [0], 42: [2], 50: [1]}\n",
    "# for i in tqdm(URL_LVL_DICT_RAD.keys()):\n",
    "#     for j in range(len(URL_LVL_DICT_RAD[i])):\n",
    "#         try:\n",
    "#             soup = BeautifulSoup(urllib.request.urlopen(URL_LVL_DICT_RAD[i][j]))\n",
    "#             a=soup.find(\"span\", class_=\"radical-icon\").text\n",
    "#             if not a:\n",
    "#                 bad_rads[i]=bad_rads.get(i,[])+[j]\n",
    "#                 link_pic = soup.find('img',class_=\"radical-image\")['src']\n",
    "#                 urllib.request.urlretrieve(link_pic,soup.head.title.text.split('/')[2].strip().split('/')[-1].lower()+'.png')\n",
    "#         except:\n",
    "#             print(i,j)\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnemonics(url,rads=False):\n",
    "    if rads:\n",
    "        return BeautifulSoup(urllib.request.urlopen(url,timeout = 120)).find('section', class_=\"mnemonic-content mnemonic-content--new\").text.replace('\\n\\n Hints\\n',' ').replace('\\n',' ').strip()\n",
    "    loc = BeautifulSoup(urllib.request.urlopen(url,timeout = 120)).find_all('section', class_=\"mnemonic-content mnemonic-content--new\")\n",
    "    return [loc[0].text.replace('\\n\\n Hints\\n',' ').replace('\\n',' ').strip(),loc[1].text.replace('\\n\\n Hints\\n',' ').replace('\\n',' ').strip()]\n",
    "def meanings(url):\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(url,timeout = 120) )\n",
    "    loc=list(map(lambda x: x.text.lower(),soup.find_all(\"p\", class_=\"text-gray-700\")))\n",
    "    newm=[]\n",
    "    for m in loc:\n",
    "        newm+=m.split(', ')\n",
    "    return newm\n",
    "def hyerogliph(url):#KAN AND VOC\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(url,timeout = 120))\n",
    "    return soup.head.title.text.split('/')[2].strip().split('/')[-1]\n",
    "def readings(url): \n",
    "    soup = BeautifulSoup(urllib.request.urlopen(url,timeout = 120))\n",
    "    loc=soup.find(\"div\", class_=\"span4 reading--kunyomi\")\n",
    "    if loc:\n",
    "        return [loc.text.replace('Kun’yomi','').strip().split(', '),\n",
    "        soup.find(\"div\", class_=\"span4 reading--onyomi muted-content\").text.replace('On’yomi','').strip().split(', '),\n",
    "        'kun']\n",
    "    else:\n",
    "        return [soup.find(\"div\", class_=\"span4 reading--kunyomi muted-content\").text.replace('Kun’yomi','').strip().split(', '),\n",
    "        soup.find(\"div\", class_=\"span4 reading--onyomi\").text.replace('On’yomi','').strip().split(', '),\n",
    "        'on']\n",
    "def context_sents(url):\n",
    "    return list(filter(None, BeautifulSoup(urllib.request.urlopen(url,timeout = 120)).find('section', id=\"context\", class_=\"context-sentence\").text.replace('Context','').replace(' Sentences','').split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DATA.pkl', 'rb') as inp:\n",
    "    DATA=pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 18/34 [04:55<04:32, 17.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='translate.google.com', port=443): Read timed out.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 33/34 [13:53<00:18, 18.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='translate.google.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:997)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [15:28<00:00, 27.32s/it]\n"
     ]
    }
   ],
   "source": [
    "#endless_kan=done!\n",
    "# from wanikani import *\n",
    "# import time\n",
    "# def endless_kan(lvl,link,wait=10):\n",
    "#     try:\n",
    "#         DATA[lvl-1]['kan'].append(kan(link,hyerogliph(link),meanings(link),*readings(link),*mnemonics(link),lvl))\n",
    "#         return\n",
    "#     except Exception as e:\n",
    "#         print(str(e))\n",
    "#         time.sleep(wait)\n",
    "#         if wait<60:\n",
    "#             wait+=1\n",
    "#         endless_kan(lvl,link,wait)\n",
    "    \n",
    "# for lvl in range(61,61):\n",
    "#     for link in tqdm(URL_LVL_DICT_KAN[lvl]):\n",
    "#         endless_kan(lvl,link)\n",
    "#     with open('DATA.pkl', 'wb') as outp:\n",
    "#         pickle.dump(DATA, outp, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/5 [00:00<?, ?it/s]\n",
      " 20%|██        | 1/5 [00:00<00:00,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The read operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [44:18<00:00, 29.54s/it]\n",
      "100%|██████████| 81/81 [34:15<00:00, 25.38s/it]t]\n",
      "100%|██████████| 88/88 [33:31<00:00, 22.86s/it]t]\n",
      "100%|██████████| 74/74 [27:46<00:00, 22.52s/it]t]\n",
      "100%|██████████| 5/5 [2:19:53<00:00, 1678.74s/it]\n"
     ]
    }
   ],
   "source": [
    "#endless_voc\n",
    "from wanikani import *\n",
    "def endless_voc(lvl,link,wait=1):\n",
    "    try:\n",
    "        if len(loc:=meanings(link))>1:\n",
    "            meanings_=loc[:-1]\n",
    "        else:\n",
    "            meanings_=loc\n",
    "        reading=list(map(lambda x:x.text.strip(), BeautifulSoup(urllib.request.urlopen(link,timeout = 120)).find_all('p', class_=\"pronunciation-variant\")))\n",
    "        DATA[lvl-1]['voc'].append(voc(link,hyerogliph(link),meanings_,reading,*mnemonics(link),context_sents(link),lvl))\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        time.sleep(wait)\n",
    "        if wait<10:\n",
    "            wait+=0.5\n",
    "        endless_voc(lvl,link,wait)\n",
    "\n",
    "for lvl in tqdm(range(56,61)):\n",
    "    for i in tqdm(range(len(DATA[lvl-1]['voc']),len(URL_LVL_DICT_VOC[lvl]))):\n",
    "        link=URL_LVL_DICT_VOC[lvl][i]\n",
    "        endless_voc(lvl,link)\n",
    "    with open('DATA.pkl', 'wb') as outp:\n",
    "        pickle.dump(DATA, outp, pickle.HIGHEST_PROTOCOL)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wanikani import *\n",
    "\n",
    "# for lvl in tqdm(range(32?,61)):\n",
    "    # for j in range(len(URL_LVL_DICT_RAD[lvl])):\n",
    "    #     link=URL_LVL_DICT_RAD[lvl][j]\n",
    "    #     meaning = BeautifulSoup(urllib.request.urlopen(link,timeout = 120)).head.title.text.split('/')[2].strip().split('/')[-1].lower()\n",
    "    #     mnem=mnemonics(link,True)\n",
    "    #     if (lvl in bad_rads) and (j in bad_rads[lvl]):\n",
    "    #         path='rads/'+meaning+'.png'\n",
    "    #         DATA[lvl-1]['rad'].append(rad(link,None,path,meaning,mnem,lvl))\n",
    "    #     else:\n",
    "    #         hye=BeautifulSoup(urllib.request.urlopen(link,timeout = 120)).find(\"span\", class_=\"radical-icon\").text.lower()\n",
    "    #         DATA[lvl-1]['rad'].append(rad(link,hye,None,meaning,mnem,lvl))\n",
    "\n",
    "    # for link in URL_LVL_DICT_KAN[lvl]:\n",
    "    #     DATA[lvl-1]['kan'].append(kan(link,hyerogliph(link),meanings(link),*readings(link),*mnemonics(link),lvl))\n",
    "    \n",
    "    # for link in URL_LVL_DICT_VOC[lvl]:\n",
    "    #     reading=list(map(lambda x:x.text.strip(), BeautifulSoup(urllib.request.urlopen(link,timeout = 120)).find_all('p', class_=\"pronunciation-variant\")))\n",
    "    #     DATA[lvl-1]['voc'].append(voc(link,hyerogliph(link),meanings(link)[:-1],reading,*mnemonics(link),context_sents(link),lvl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DATA.pkl', 'wb') as outp:\n",
    "    pickle.dump(DATA, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " https://files.wanikani.com/xg8ygnendcn6on94dwbl3t0yq2w9\n"
     ]
    }
   ],
   "source": [
    "#rad_hyerogliph/image\n",
    "# f = urllib.request.urlopen(URL_LVL_DICT_RAD[1][8]) \n",
    "# soup = BeautifulSoup(f)\n",
    "# a=soup.find(\"span\", class_=\"radical-icon\").text\n",
    "# link_pic=''\n",
    "# if not a:\n",
    "#     link_pic = soup.find('img',class_=\"radical-image\")['src']\n",
    "#     name='gun.png'\n",
    "#     urllib.request.urlretrieve(link_pic,name)\n",
    "# print(a,link_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abundant', 'plentiful']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#meanings\n",
    "# f = urllib.request.urlopen(URL_LVL_DICT_KAN[20][0]) \n",
    "# soup = BeautifulSoup(f)\n",
    "# list(map(lambda x: x.text.lower(),soup.find_all(\"p\", class_=\"text-gray-700\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一人'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hierogliphs for KAN and VOC!\n",
    "# f = urllib.request.urlopen(URL_LVL_DICT_VOC[1][1]) \n",
    "# soup = BeautifulSoup(f)\n",
    "# soup.head.title.text.split('/')[2].strip().split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When there's one person, what are they? Well, they're either just going to be simply one person or alone.\n",
      "-----------------------------------------------------\n",
      "The reading for this vocab doesn't follow any rules you learned previously. It's an exception, which means you have to learn the reading separately too. Spend a few moments trying to remember this word, look away for 10 seconds, and then try to recall its reading. Could you do it? Try again, this time in thirty seconds. Did you do it again? Okay, go ahead and move on. You may know it now, though you'll need to recall it again in the next 5-10 minutes if you want to remember it for good. Alternatively, if you know the reading for the vocab word 一つ (in the same level as this word), you can know that it uses the same reading!\n"
     ]
    }
   ],
   "source": [
    "#mnem_meaning _ reading\n",
    "# f = urllib.request.urlopen(URL_LVL_DICT_VOC[1][1]) \n",
    "# soup = BeautifulSoup(f)\n",
    "# l=soup.find_all('section', class_=\"mnemonic-content mnemonic-content--new\")\n",
    "# print(l[0].text.replace('\\n\\n Hints\\n',' ').replace('\\n',' ').strip())\n",
    "# print('-----------------------------------------------------')\n",
    "# print(l[1].text.replace('\\n\\n Hints\\n',' ').replace('\\n',' ').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WaniKani / Vocabulary / ハチの巣\n",
      "はちのす\n",
      "ハチのす\n"
     ]
    }
   ],
   "source": [
    "#readings _ VOC\n",
    "# f = urllib.request.urlopen(URL_LVL_DICT_VOC[31][8]) \n",
    "# soup = BeautifulSoup(f)\n",
    "# print(soup.head.title.text)\n",
    "# for i in soup.find_all('p', class_=\"pronunciation-variant\"):\n",
    "#     print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WaniKani / Kanji / 越\n",
      "えつ\n"
     ]
    }
   ],
   "source": [
    "#main_readings\n",
    "# f = urllib.request.urlopen(URL_LVL_DICT_KAN[30][10]) \n",
    "# soup = BeautifulSoup(f)\n",
    "# print(soup.head.title.text)\n",
    "# for i in soup.find_all(\"div\", class_=\"span4 reading--onyomi\"):\n",
    "#     print(i.text.replace('On’yomi','').strip())\n",
    "# for i in soup.find_all(\"div\", class_=\"span4 reading--kunyomi\"):\n",
    "#     print(i.text.replace('Kun’yomi','').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WaniKani / Kanji / 越\n",
      "こ\n"
     ]
    }
   ],
   "source": [
    "#secondary_readings\n",
    "# f = urllib.request.urlopen(URL_LVL_DICT_KAN[30][10]) \n",
    "# soup = BeautifulSoup(f)\n",
    "# print(soup.head.title.text)\n",
    "# for i in soup.find_all(\"div\", class_=\"span4 reading--kunyomi muted-content\"):\n",
    "#     print(i.text.replace('Kun’yomi','').strip())\n",
    "# for i in soup.find_all(\"div\", class_=\"span4 reading--onyomi muted-content\"):\n",
    "#     print(i.text.replace(\"On’yomi\",'').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WaniKani / Vocabulary / 大人\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nContext\\n\\n\\n\\nContext Sentences\\n\\nこれは、大人のりょうきんです。\\nThis is the adult price.\\n\\n\\n大人は三人だけです。\\nThere are only three adults.\\n\\n\\n大人たちはいざかやにいった。\\nThe adults went to an izakaya.\\n\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#contexrt sents\n",
    "# f = urllib.request.urlopen(URL_LVL_DICT_VOC[1][0]) \n",
    "# soup = BeautifulSoup(f)\n",
    "# print(soup.head.title.text)\n",
    "# soup.find('section', id=\"context\", class_=\"context-sentence\").text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
