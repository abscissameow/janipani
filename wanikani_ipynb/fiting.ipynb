{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from wanikani import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIC_RAD={}\n",
    "# DIC_KAN={}\n",
    "# DIC_VOC={}\n",
    "# DIC_ALL={}\n",
    "# DIC_NAMES={}\n",
    "with open('FIT_DATAS.pkl', 'rb') as inp:\n",
    "    DIC_RAD,DIC_KAN,DIC_VOC,DIC_ALL,DIC_NAMES,accLVL,accReview_from=pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE=0\n",
    "LVL=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PROCESSING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_rads(line):\n",
    "    line=line.replace('\\n','')\n",
    "    meanings,mnemonics=line.split('.',1)\n",
    "    meanings = (lambda x: [i.lower() for i in x])(meanings.split(','))\n",
    "    for i in range(len(meanings)):\n",
    "        if meanings[i][0]==' ':\n",
    "            meanings[i]=meanings[i][1:]\n",
    "        if meanings[i][-1]==' ':\n",
    "            meanings[i]=meanings[i][:-1]\n",
    "    if mnemonics[0]==' ':\n",
    "        mnemonics=mnemonics[1:]\n",
    "    return meanings,mnemonics\n",
    "def handle_not_rads(line):\n",
    "    line=line.replace('\\n','')\n",
    "    meanings,readings,mnemonics = line.split(\".\", 2)\n",
    "    meanings = (lambda x: [i.lower() for i in x])(meanings.split(','))\n",
    "    readings = readings.split(',')\n",
    "    for i in range(len(meanings)):\n",
    "        if meanings[i][0]==' ':\n",
    "            meanings[i]=meanings[i][1:]\n",
    "        if meanings[i][-1]==' ':\n",
    "            meanings[i]=meanings[i][:-1]\n",
    "    if mnemonics[0]==' ':\n",
    "        mnemonics=mnemonics[1:]\n",
    "    for i in range(len(readings)):\n",
    "        if readings[i][0]==' ':\n",
    "            readings[i]=readings[i][1:]\n",
    "        if readings[i][-1]==' ':\n",
    "            readings[i]=readings[i][:-1]\n",
    "    return meanings,readings,mnemonics\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "with open('rads/lvl'+str(LVL)+'/data.txt', 'r') as file:\n",
    "    data = file.readlines()\n",
    "RADS_DATA={}\n",
    "for i in data:\n",
    "    # data[i]=data[i].replace('\\n','')\n",
    "    # meanings,mnemonics = data[i].split(\".\", 1)\n",
    "    # meanings = (lambda x: [i.lower() for i in x])(meanings.split(','))\n",
    "    meanings,mnemonics=handle_rads(i)\n",
    "    RADS_DATA[meanings[0]] = [meanings, mnemonics]\n",
    "    \n",
    "with open('kans/lvl'+str(LVL)+'/data.txt', 'r') as file:\n",
    "    data = file.readlines()\n",
    "KANS_DATA={}\n",
    "for i in data:\n",
    "    # data[i]=data[i].replace('\\n','')\n",
    "    # meanings,readings,mnemonics = data[i].split(\".\", 2)\n",
    "    # meanings = (lambda x: [i.lower() for i in x])(meanings.split(','))\n",
    "    meanings,readings,mnemonics = handle_not_rads(i)\n",
    "    KANS_DATA[meanings[0]] = [meanings,readings, mnemonics]\n",
    "\n",
    "with open('vocs/lvl'+str(LVL)+'/data.txt', 'r') as file:\n",
    "    data = file.readlines()\n",
    "VOCS_DATA={}\n",
    "for i in range(len(data)):\n",
    "    data[i]=data[i].replace('\\n','')\n",
    "    meanings,readings,mnemonics = data[i].split(\".\", 2)\n",
    "    meanings = (lambda x: [i.lower() for i in x])(meanings.split(','))\n",
    "    VOCS_DATA[meanings[0]] = [meanings,readings.split(','), mnemonics] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MATCHING DATA+PICTURES AND INSERTING INTO THE DICTIONARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('rads/lvl'+str(LVL)):\n",
    "    for filename in filenames:\n",
    "            filepath = os.path.join(dirname, filename)\n",
    "            if (filename!='.DS_Store')and(filename!='data.txt')and(('rad',filename[:-4].lower()) not in DIC_NAMES):\n",
    "                #stage, pic, meaning, mnemonics='obvious', lvl=0, previous_time=time.time()\n",
    "                item=rad(LVL,filepath.lower(),RADS_DATA[filename[:-4].lower()][0],RADS_DATA[filename[:-4].lower()][1],STAGE)\n",
    "                DIC_RAD[LVL]=DIC_RAD.get(LVL,[])+[item]\n",
    "                DIC_ALL[LVL]=DIC_ALL.get(LVL,[])+[item]\n",
    "                DIC_NAMES[('rad',filename[:-4].lower())]=item\n",
    "\n",
    "for dirname, _, filenames in os.walk('kans/lvl'+str(LVL)):\n",
    "    for filename in filenames:\n",
    "            filepath = os.path.join(dirname, filename)\n",
    "            if (filename!='.DS_Store')and(filename!='data.txt')and(('kan',filename[:-4].lower()) not in DIC_NAMES):\n",
    "                data=KANS_DATA[filename[:-4].lower()]\n",
    "                #stage, pic, meanings, readings, mnemonics='obvious', lvl=0, previous_time=time.time()\n",
    "                item=kan(LVL,filepath.lower(),data[0],data[1],data[2],STAGE)\n",
    "                DIC_KAN[LVL]=DIC_KAN.get(LVL,[])+[item]\n",
    "                DIC_ALL[LVL]=DIC_ALL.get(LVL,[])+[item]\n",
    "                DIC_NAMES[('kan',filename[:-4].lower())]=item\n",
    "\n",
    "for dirname, _, filenames in os.walk('vocs/lvl'+str(LVL)):\n",
    "    for filename in filenames:\n",
    "            filepath = os.path.join(dirname, filename)\n",
    "            if (filename!='.DS_Store')and(filename!='data.txt')and(('voc',filename[:-4].lower()) not in DIC_NAMES):\n",
    "                data=VOCS_DATA[filename[:-4].lower()]\n",
    "                #stage, pic, meanings, readings, mnemonics='obvious',lvl=0,previous_time=time.time()\n",
    "                item=voc(LVL,filepath.lower(),data[0],data[1],data[2],STAGE)\n",
    "                DIC_VOC[LVL]=DIC_VOC.get(LVL,[])+[item]\n",
    "                DIC_ALL[LVL]=DIC_ALL.get(LVL,[])+[item]\n",
    "                DIC_NAMES[('voc',filename[:-4].lower())]=item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIC_NAMES[('voc','baby')].show()\n",
    "DIC_NAMES[('voc','another person')].info()\n",
    "time.sleep(5)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVE CHANGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIT_DATAS=[DIC_RAD,DIC_KAN,DIC_VOC,DIC_ALL,DIC_NAMES,accLVL,accReview_from]\n",
    "with open('FIT_DATAS.pkl', 'wb') as outp:\n",
    "    pickle.dump(FIT_DATAS, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
